import os
from fastapi import FastAPI
from pydantic import BaseModel
from typing import Dict, Any

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import AzureChatOpenAI
from langchain_community.retrievers import AzureAISearchRetriever

# Initialize the FastAPI application
app = FastAPI(
    title="SCA Chatbot API",
    description="An API to answer material price questions using Azure AI Search and Azure OpenAI.",
    version="1.0.0",
)

# Use a Pydantic BaseModel to define the structure of the request body.
class QueryRequest(BaseModel):
    """
    Request model for the chatbot query.
    """
    question: str


# --- Azure and Langchain Setup ---
# Note: For production, it is highly recommended to manage these credentials
# using environment variables or a secret management service.

# Step 2: Set the environment variables for Azure AI Search
# These variables configure the search service and index for retrieving documents
AZURE_AI_SEARCH_SERVICE_NAME=os.environ["AZURE_AI_SEARCH_SERVICE_NAME"]
AZURE_AI_SEARCH_INDEX_NAME=os.environ["AZURE_AI_SEARCH_INDEX_NAME"]
AZURE_AI_SEARCH_API_KEY=os.environ["AZURE_AI_SEARCH_API_KEY"]
AZURE_OPENAI_API_KEY=os.environ["AZURE_OPENAI_API_KEY"]
AZURE_OPENAI_ENDPOINT=os.environ["AZURE_OPENAI_ENDPOINT"]

# Step 1: Initialize the AzureAI Search Retriever
# This retrieves relevant documents based on the user query from the Azure Search index
retriever = AzureAISearchRetriever(
    api_key=AZURE_AI_SEARCH_API_KEY,
    service_name=AZURE_AI_SEARCH_SERVICE_NAME,
    index_name=AZURE_AI_SEARCH_INDEX_NAME,
    content_key="content",
    top_k=3
)

# Step 2: Define the prompt template for the language model
# This sets up how the context and question will be formatted for the model
prompt = ChatPromptTemplate.from_template(
    """
You are an assistant that answers material price questions using the dataset only (2013â€“2025, actual & predicted values).

Rules:
- Use only the provided context. Do not invent data.
- Always include other relevant factors (gold, oil, shares) if available.
- If a price for a month is missing, estimate it from nearby months and explain clearly that it is an estimate.
- If no data is found at all, reply: "Sorry, I dont have information about that."

Context:
{context}

Question:
{question}

Answer:
"""
)

# Step 3: Initialize the Azure Chat OpenAI model
# This sets up the model to be used for generating responses
llm = AzureChatOpenAI(
    api_version="2025-01-01-preview",
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_API_KEY,
    model="gpt-4.1",
    temperature=0.3,
    max_tokens=150
)

# Step 4: Create a processing chain
# This chain will process the retrieved context and the user question
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# --- FastAPI Endpoint ---

@app.post("/ask", response_model=Dict[str, Any])
async def ask_question(request: QueryRequest):
    """
    Accepts a user question and returns an answer generated by the LLM
    using Azure AI Search for context.
    """
    try:
        response = await chain.ainvoke(request.question)
        return {"answer": response}
    except Exception as e:
        return {"error": str(e)}

# Note: The code to run this server is not part of this file.
# You will run it from your terminal using the 'uvicorn' command.
